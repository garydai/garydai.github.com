<!DOCTYPE html>


  <html class="light page-default">


<head>
  <meta charset="utf-8">
  
  <title>scrapy | Hexo</title>

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
    <meta name="keywords" content="后端、技术、博客、架构、全栈" />
  

  <meta name="description" content="##scrapy 单进程，基于twisted(利用epoll或其他多路复用) 流程 settings &#x3D; conf.settings inproject &#x3D; inside_project() cmds &#x3D; _get_commands_dict(settings, inproject) cmdname &#x3D; _pop_command_name(argv) cmd &#x3D; cmds[cmdname] ...">
<meta property="og:type" content="article">
<meta property="og:title" content="scrapy">
<meta property="og:url" content="http://yoursite.com/2015/02/15/2015-2-15-scrapy/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="##scrapy 单进程，基于twisted(利用epoll或其他多路复用) 流程 settings &#x3D; conf.settings inproject &#x3D; inside_project() cmds &#x3D; _get_commands_dict(settings, inproject) cmdname &#x3D; _pop_command_name(argv) cmd &#x3D; cmds[cmdname] ...">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/garydai/garydai.github.com/raw/master/_posts/pic/scrapy.jpg">
<meta property="article:published_time" content="2015-02-14T16:00:00.000Z">
<meta property="article:modified_time" content="2020-01-06T07:17:32.497Z">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/garydai/garydai.github.com/raw/master/_posts/pic/scrapy.jpg">

  

  
    <link rel="icon" href="/favicon.ico">
  

  <link href="/css/styles.css?v=c114cbeddx" rel="stylesheet">


  
    
<link rel="stylesheet" href="/css/personal-style.css">

  

  

  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?65e0b4df283d78e15f8c78f238ec9593";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  
  <script type="text/javascript">
	(function(){
	    var bp = document.createElement('script');
	    var curProtocol = window.location.protocol.split(':')[0];
	    if (curProtocol === 'https') {
	        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
	    }
	    else {
	        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
	    }
	    var s = document.getElementsByTagName("script")[0];
	    s.parentNode.insertBefore(bp, s);
	})();
  </script>



  
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

<meta name="generator" content="Hexo 4.2.0"></head>

<body>


  
    <span id="toolbox-mobile" class="toolbox-mobile">导航</span>
  

  <div class="post-header CENTER">
   
  <div class="toolbox">
    <a class="toolbox-entry" href="/">
      <span class="toolbox-entry-text">导航</span>
      <i class="icon-angle-down"></i>
      <i class="icon-home"></i>
    </a>
    <ul class="list-toolbox">
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/archives/"
            rel="noopener noreferrer"
            target="_self"
            >
            博客
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/category/"
            rel="noopener noreferrer"
            target="_self"
            >
            分类
          </a>
        </li>
      
        <li class="item-toolbox">
          <a
            class="CIRCLE"
            href="/tag/"
            rel="noopener noreferrer"
            target="_self"
            >
            标签
          </a>
        </li>
      
    </ul>
  </div>


</div>




<div class="content content-post CENTER">
   <article id="default-2015-2-15-scrapy" class="article article-type-default" itemprop="blogPost">
  <header class="article-header">
    <h1 class="post-title">scrapy</h1>

    <div class="article-meta">
      <span>
        <i class="icon-calendar"></i>
        <span>2015.02.15</span>
      </span>

      
        <span class="article-author">
          <i class="icon-user"></i>
          <span>John Doe</span>
        </span>
      

      


      

      
      <i class="fa fa-eye"></i> 
        <span id="busuanzi_container_page_pv">
           &nbsp热度 <span id="busuanzi_value_page_pv">
           <i class="fa fa-spinner fa-spin"></i></span>℃
        </span>
      
      
    </div>
  </header>

  <div class="article-content">
    
      <p>##scrapy</p>
<p>单进程，基于twisted(利用epoll或其他多路复用)</p>
<p>流程</p>
<pre><code>settings = conf.settings
inproject = inside_project()
cmds = _get_commands_dict(settings, inproject)
cmdname = _pop_command_name(argv)
cmd = cmds[cmdname]
...
cmd.crawler_process = CrawlerProcess(settings)
cmd.run()</code></pre><p>1.读取scrapy.cgf配置文件</p>
<pre><code>[settings]
default = m_p.settings

[deploy]
#url = http://localhost:6800/
project = m_p</code></pre><p>2.读取setting设置文件</p>
<pre><code>BOT_NAME = &apos;m_p&apos;

SPIDER_MODULES = [&apos;m_p.spiders&apos;]
NEWSPIDER_MODULE = &apos;m_p.spiders&apos;

ITEM_PIPELINES = {&apos;m_p.pipelines.mysql_pipeline&apos;:2, &apos;m_p.pipelines.download_image_pipeline&apos;:1 }

SERVER = &quot;localhost&quot;
PORT = 3306
DB = &quot;music&quot;
COLLECTION = &quot;music&quot;
USER = &apos;root&apos;
PASSWORD = &apos;&apos;
CHARSET = &apos;utf8&apos;
IMAGES_STORE=&apos;/home/admin/nginx/html/yii/demos/music_web/images/cover/&apos;</code></pre><p>3.导入相应的module爬虫模块</p>
<p>4.解析命令行参数</p>
<p>5.设置爬虫程序入口</p>
<p>6.运行cmd</p>
<pre><code>def run(self, args, opts):
     if len(args) &lt; 1:
         raise UsageError()
     elif len(args) &gt; 1:
         raise UsageError(&quot;running &apos;scrapy crawl&apos; with more than one spider is no longer supported&quot;)
     spname = args[0]

     self.crawler_process.crawl(spname, **opts.spargs)
     self.crawler_process.start()




 crawler.py  

 class CrawlerRunner(object):

 def crawl(self, spidercls, *args, **kwargs):
     crawler = self._create_crawler(spidercls)
     self._setup_crawler_logging(crawler)
     self.crawlers.add(crawler)
     d = crawler.crawl(*args, **kwargs)
     self._active.add(d)

     def _done(result):
         self.crawlers.discard(crawler)
         self._active.discard(d)
         return result

     return d.addBoth(_done)


 class Crawler(object):


 def crawl(self, *args, **kwargs):
     assert not self.crawling, &quot;Crawling already taking place&quot;
     self.crawling = True

     try:
         self.spider = self._create_spider(*args, **kwargs)
         self.engine = self._create_engine()
         start_requests = iter(self.spider.start_requests())
         yield self.engine.open_spider(self.spider, start_requests)  open engine&apos;s spider
         yield defer.maybeDeferred(self.engine.start)
     except Exception:
         self.crawling = False
         raise


 class ExecutionEngine(object):

 @defer.inlineCallbacks
 def open_spider(self, spider, start_requests=(), close_if_idle=True):
     assert self.has_capacity(), &quot;No free spider slot when opening %r&quot; % \
         spider.name
     log.msg(&quot;Spider opened&quot;, spider=spider)
     nextcall = CallLaterOnce(self._next_request, spider)
     scheduler = self.scheduler_cls.from_crawler(self.crawler)
     start_requests = yield self.scraper.spidermw.process_start_requests(start_requests, spider)
     slot = Slot(start_requests, close_if_idle, nextcall, scheduler)
     self.slot = slot
     self.spider = spider
     yield scheduler.open(spider)
     yield self.scraper.open_spider(spider)
     self.crawler.stats.open_spider(spider)
     yield self.signals.send_catch_log_deferred(signals.spider_opened, spider=spider)
     slot.nextcall.schedule()


 class CrawlerProcess(CrawlerRunner):

 def start(self, stop_after_crawl=True):
     if stop_after_crawl:
         d = self.join()
         # Don&apos;t start the reactor if the deferreds are already fired
         if d.called:
             return
         d.addBoth(lambda _: self._stop_reactor())

     if self.settings.getbool(&apos;DNSCACHE_ENABLED&apos;):
         reactor.installResolver(CachingThreadedResolver(reactor))

     reactor.addSystemEventTrigger(&apos;before&apos;, &apos;shutdown&apos;, self.stop)
     reactor.run(installSignalHandlers=False)  # blocking call</code></pre><p>用了Twisted-&gt;reactor</p>
<p><img src="https://github.com/garydai/garydai.github.com/raw/master/_posts/pic/scrapy.jpg" alt=""></p>
<p>1.engine从spider获取要爬取的url</p>
<p>2.engine将url交给调度器</p>
<p>3.engine从调度器获取要爬取的url</p>
<p> _next_request_from_scheduler -&gt;slot.scheduler.next_request()</p>
<pre><code>def _next_request_from_scheduler(self, spider):
     slot = self.slot
     request = slot.scheduler.next_request()
     if not request:
         return
     d = self._download(request, spider)
     d.addBoth(self._handle_downloader_output, request, spider)
     d.addErrback(log.msg, spider=spider)
     d.addBoth(lambda _: slot.remove_request(request))
     d.addErrback(log.msg, spider=spider)
     d.addBoth(lambda _: slot.nextcall.schedule())
     d.addErrback(log.msg, spider=spider)
     return d</code></pre><p>4.engine将url通过下载中间件交给下载器</p>
<pre><code>slot.add_request(request)

    dwld = self.downloader.fetch(request, spider)
    dwld.addCallbacks(_on_success)
    dwld.addBoth(_on_complete)</code></pre><p>5.下载完毕后，下载器将response交给engine</p>
<p>6.engine将response通过spider中间件交给spider</p>
<p>7.spider处理response，将item和下一个请求url返回给engine</p>
<p>8.engine将item交给item pipeline,将url交给调度器</p>
<p>9.第2步</p>
<pre><code>class CrawlerProcess(CrawlerRunner):
    &quot;&quot;&quot;A class to run multiple scrapy crawlers in a process simultaneously&quot;&quot;&quot;

    def __init__(self, settings):
        super(CrawlerProcess, self).__init__(settings)
        install_shutdown_handlers(self._signal_shutdown)
        self.stopping = False
        self.log_observer = log.start_from_settings(self.settings)
        log.scrapy_info(settings)

    def _signal_shutdown(self, signum, _):
        install_shutdown_handlers(self._signal_kill)
        signame = signal_names[signum]
        log.msg(format=&quot;Received %(signame)s, shutting down gracefully. Send again to force &quot;,
                level=log.INFO, signame=signame)
        reactor.callFromThread(self.stop)

    def _signal_kill(self, signum, _):
        install_shutdown_handlers(signal.SIG_IGN)
        signame = signal_names[signum]
        log.msg(format=&apos;Received %(signame)s twice, forcing unclean shutdown&apos;,
                level=log.INFO, signame=signame)
        self._stop_logging()
        reactor.callFromThread(self._stop_reactor)

    def start(self, stop_after_crawl=True):
        if stop_after_crawl:
            d = self.join()
            # Don&apos;t start the reactor if the deferreds are already fired
            if d.called:
                return
            d.addBoth(lambda _: self._stop_reactor())

        if self.settings.getbool(&apos;DNSCACHE_ENABLED&apos;):
            reactor.installResolver(CachingThreadedResolver(reactor))

        reactor.addSystemEventTrigger(&apos;before&apos;, &apos;shutdown&apos;, self.stop)
        reactor.run(installSignalHandlers=False)  # blocking call

    def _stop_logging(self):
        if self.log_observer:
            self.log_observer.stop()

    def _stop_reactor(self, _=None):
        try:
            reactor.stop()
        except RuntimeError:  # raised if already stopped or in shutdown stage
            pass



class CrawlerRunner(object):

    def __init__(self, settings):
        self.settings = settings
        smcls = load_object(settings[&apos;SPIDER_MANAGER_CLASS&apos;])
        self.spiders = smcls.from_settings(settings.frozencopy())
        self.crawlers = set()
        self._active = set()

    def crawl(self, spidercls, *args, **kwargs):
        crawler = self._create_crawler(spidercls)
        self._setup_crawler_logging(crawler)
        self.crawlers.add(crawler)
        d = crawler.crawl(*args, **kwargs)
        self._active.add(d)

        def _done(result):
            self.crawlers.discard(crawler)
            self._active.discard(d)
            return result

        return d.addBoth(_done)

    def _create_crawler(self, spidercls):
        if isinstance(spidercls, six.string_types):
            spidercls = self.spiders.load(spidercls)

        crawler_settings = self.settings.copy()
        spidercls.update_settings(crawler_settings)
        crawler_settings.freeze()
        return Crawler(spidercls, crawler_settings)

    def _setup_crawler_logging(self, crawler):
        log_observer = log.start_from_crawler(crawler)
        if log_observer:
            crawler.signals.connect(log_observer.stop, signals.engine_stopped)

    def stop(self):
        return defer.DeferredList([c.stop() for c in list(self.crawlers)])

    @defer.inlineCallbacks
    def join(self):
        &quot;&quot;&quot;Wait for all managed crawlers to complete&quot;&quot;&quot;
        while self._active:
            yield defer.DeferredList(self._active)</code></pre>
    
  </div>

</article>


   

   
  <div class="box-prev-next clearfix">
    <a class="show pull-left" href="/2015/02/15/2015-2-15-Twisted/">
        <i class="icon icon-angle-left"></i>
    </a>
    <a class="show pull-right" href="/2015/02/15/2015-2-15-python%E9%97%AD%E5%8C%85/">
        <i class="icon icon-angle-right"></i>
    </a>
  </div>




</div>


  <a id="backTop" class="back-top">
    <i class="icon-angle-up"></i>
  </a>




  <div class="modal" id="modal">
  <span id="cover" class="cover hide"></span>
  <div id="modal-dialog" class="modal-dialog hide-dialog">
    <div class="modal-header">
      <span id="close" class="btn-close">Close</span>
    </div>
    <hr>
    <div class="modal-body">
      <ul class="list-toolbox">
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/archives/"
              rel="noopener noreferrer"
              target="_self"
              >
              博客
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/category/"
              rel="noopener noreferrer"
              target="_self"
              >
              分类
            </a>
          </li>
        
          <li class="item-toolbox">
            <a
              class="CIRCLE"
              href="/tag/"
              rel="noopener noreferrer"
              target="_self"
              >
              标签
            </a>
          </li>
        
      </ul>

    </div>
  </div>
</div>



  
      <div class="fexo-comments comments-default">
    

    

    
    

    

    
    

    

  </div>

  

  <script type="text/javascript">
  function loadScript(url, callback) {
    var script = document.createElement('script')
    script.type = 'text/javascript';

    if (script.readyState) { //IE
      script.onreadystatechange = function() {
        if (script.readyState == 'loaded' ||
          script.readyState == 'complete') {
          script.onreadystatechange = null;
          callback();
        }
      };
    } else { //Others
      script.onload = function() {
        callback();
      };
    }

    script.src = url;
    document.getElementsByTagName('head')[0].appendChild(script);
  }

  window.onload = function() {
    loadScript('/js/bundle.js?235683', function() {
      // load success
    });
  }
</script>

</body>
</html>
