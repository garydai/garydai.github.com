---
layout: default
title: scrapy

---

##scrapy

用了Twisted->reactor



![](https://github.com/garydai/garydai.github.com/raw/master/_posts/pic/scrapy.jpg)

1.engine从spider获取要爬取的url

2.engine将url交给调度器

3.engine从调度器获取要爬取的url

4.engine将url通过下载中间件交给下载器

5.下载完毕后，下载器将response交给engine

6.engine将response通过spider中间件交给spider

7.spider处理response，将item和下一个请求url返回给engine

8.engine将item交给item pipeline,将url交给调度器

9.第2步



	class CrawlerProcess(CrawlerRunner):
	    """A class to run multiple scrapy crawlers in a process simultaneously"""
	
	    def __init__(self, settings):
	        super(CrawlerProcess, self).__init__(settings)
	        install_shutdown_handlers(self._signal_shutdown)
	        self.stopping = False
	        self.log_observer = log.start_from_settings(self.settings)
	        log.scrapy_info(settings)
	
	    def _signal_shutdown(self, signum, _):
	        install_shutdown_handlers(self._signal_kill)
	        signame = signal_names[signum]
	        log.msg(format="Received %(signame)s, shutting down gracefully. Send again to force ",
	                level=log.INFO, signame=signame)
	        reactor.callFromThread(self.stop)
	
	    def _signal_kill(self, signum, _):
	        install_shutdown_handlers(signal.SIG_IGN)
	        signame = signal_names[signum]
	        log.msg(format='Received %(signame)s twice, forcing unclean shutdown',
	                level=log.INFO, signame=signame)
	        self._stop_logging()
	        reactor.callFromThread(self._stop_reactor)
	
	    def start(self, stop_after_crawl=True):
	        if stop_after_crawl:
	            d = self.join()
	            # Don't start the reactor if the deferreds are already fired
	            if d.called:
	                return
	            d.addBoth(lambda _: self._stop_reactor())
	
	        if self.settings.getbool('DNSCACHE_ENABLED'):
	            reactor.installResolver(CachingThreadedResolver(reactor))
	
	        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)
	        reactor.run(installSignalHandlers=False)  # blocking call
	
	    def _stop_logging(self):
	        if self.log_observer:
	            self.log_observer.stop()
	
	    def _stop_reactor(self, _=None):
	        try:
	            reactor.stop()
	        except RuntimeError:  # raised if already stopped or in shutdown stage
	            pass



	class CrawlerRunner(object):
	
	    def __init__(self, settings):
	        self.settings = settings
	        smcls = load_object(settings['SPIDER_MANAGER_CLASS'])
	        self.spiders = smcls.from_settings(settings.frozencopy())
	        self.crawlers = set()
	        self._active = set()
	
	    def crawl(self, spidercls, *args, **kwargs):
	        crawler = self._create_crawler(spidercls)
	        self._setup_crawler_logging(crawler)
	        self.crawlers.add(crawler)
	        d = crawler.crawl(*args, **kwargs)
	        self._active.add(d)
	
	        def _done(result):
	            self.crawlers.discard(crawler)
	            self._active.discard(d)
	            return result
	
	        return d.addBoth(_done)
	
	    def _create_crawler(self, spidercls):
	        if isinstance(spidercls, six.string_types):
	            spidercls = self.spiders.load(spidercls)
	
	        crawler_settings = self.settings.copy()
	        spidercls.update_settings(crawler_settings)
	        crawler_settings.freeze()
	        return Crawler(spidercls, crawler_settings)
	
	    def _setup_crawler_logging(self, crawler):
	        log_observer = log.start_from_crawler(crawler)
	        if log_observer:
	            crawler.signals.connect(log_observer.stop, signals.engine_stopped)
	
	    def stop(self):
	        return defer.DeferredList([c.stop() for c in list(self.crawlers)])
	
	    @defer.inlineCallbacks
	    def join(self):
	        """Wait for all managed crawlers to complete"""
	        while self._active:
	            yield defer.DeferredList(self._active)
	

